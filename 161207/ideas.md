# Что такое тестирование и зачем оно
Тестирование – это технологический процесс, направленный на выявление ошибок.

## Какие бывают ошибки
### Опечатка
<= вместо <, не то название переменной (в динамических языках). Опечатались.
### Некорректный алгоритм (неверный инвариант)
Не доказали.
### Неверное использование сторонней функции
Думали, что делает одно, а на самом деле делает другое (не угадали с интерфейсом).
### Остатки от изменения
Не поменяли в копипасте, "проблема последней строки"
### Специфичное: утечки памяти
### Специфичное: утечки ресурсов
### Специфичное: многопоточная синхронизация
### Не соответствует заданию
### Задание не соответствует реальности
### Задание не соответствует здравому смыслу (привет из UX)

Показать, что это важно не только в алгоритмах, но и в проектах (когда будут писать свой код).

Показывают только наличие багов, но не их отсутствие. Зато могут минимизировать тупые баги.

Бывает даже TDD: сначала пишем тесты, потом реализацию.

# Тестирование
## Статическое (проверка типов, стиль кода и прочее)
Можно сюда же и автоматическое доказательство впихнут, но очень сложно.
## Написание тестов и проверка поведения программы на них
Практически никогда не покроет все случаи, поэтому надеемся, что мы берём самую "сложную" выборку тестов.

## Программист и тестировщик
https://habrahabr.ru/post/149903/
Unit-тесты пишет разработчик, который знает устройство кода, и собственно организует код так, чтобы можно было писать тесты.

Функциональными тестами занимаются тестировщики, т. е. те кто знаком с предметной областью, но не обязательно
знаком с кодом. При создании функциональных тестов может потребоваться настройка окружения и т.д. и т.п.
Функциональные тесты можно условно разбить на:
 - тестирование соответствия решения задаче
 - тестирование стабильности (вот где появляются в основном рандомные тесты)
 - здесь же могут появляться тесты производительности (хотя они могут появляться и не только здесь)

 "это не вина тестировщика, что он нашел ошибку" - понятно, но я не очень хорошо понимаю на практике, "чем занимается разработчик, а чем занимается тестировщик". Могу предположить, что первый пишет код и автоматические тесты к своему коду, а второй ищет баги, к которым тесты фиг напишешь или же сам пишет тесты. Или тут что-то про white box/black box? Или лучше вообще не лезть?


См. выше, но плюс минус суть сводится к тому, что тестировщики знают какую задачу решает продукт и исходя из этого
составляют тесты, в то время как разработчики знают как код устроен внутри и составляют тесты опираясь на это знание.

Ну и да, некоторые тесты разработчики не автоматизируют, например, зачастую за бортом остаются UI тесты (хорошо,
когда есть тулы вроде Selenium, но когда их нет, то на такого рода работу часто натравливают тестировщиков).

# Какие тесты бывают

## По запуску
### Ручное тестирование
Всякий UI и "по умолчанию".
### Автоматическое тестирование
Запускается мгновенно, обычно есть фреймворк.
Можно встроить в CI и запускать постоянно, мгновенно обнаруживая ошибки.
### Встроенные в код assert'ы
Падают сразу, но недопустимы в серьёзных системах вроде кода ядра, потому что стабильность. Вырезать их из production-кода тоже нехорошо. Надо балансировать.

## По размеру и гранулярности тестируемого кода
### Unit-тесты
Отдельный код и интерфейсы функций, классов.
Запускаются быстро и не имеют зависимостей.
Mock'и во все поля.
### Интеграционные тесты
Тестируют код между несколькими юнитами.
### Функциональные тесты
Тестируют систему сверху донизу.

## По типу
white box, black box

## По целям
### Сценариев поведения
* Просто среднестатистическое использование: создали объект, что-то сделали, проверили.
* Крайние случаи
* Обработка ошибок
### Регрессионные
На старые баги
### Проверка типов в динамических языках
### Соответствие решения задаче
Только black box, проверяем глобально интерфейс, могут писать тестировщики.
### Стабильность
Стресс-тест, рандомный ввод
### Скорость работы
Линия, а не квадрат. Можно графики строить.

## По качеству
### Good
### Flaky, unstable, вероятностные

# Как писать автотесты
## Зачем
* Огромное число багов ловится тестами, которые легко написать.
* Документируют и фиксируют ожидаемое поведение (привет из гугла - там если к коду тестов нет, то их пишут, просто чтобы проверить, что ничего не поломали при изменениях).
* Если вбиваете на каждый запуск что-то - очевидно, что можно автоматизировать и будет во всяком случае быстрее.
* При тестировании по отдельности больше шанс поймать баг.
* Обнажаются изъяны архитектуры: если что-то сложно протестировать, то там наверняка прячется куча зависимостей и инвариантов.
## Мультитест в олимпиадах
Проще всего писать, толще всего (как бы функциональные).
Показали ввод - получили вывод. Написали скрипт, который запустит код X раз на всех тестах, проверит ответы.
На олимпиадах вы постоянно запускаете решение на конкретном тесте, поэтому - мультитест и diff на файлик с ответом (потому что управлять несколькими файликами сложнее).
Обычно возникают проблемы (все хорошо лечатся): читать до EOF, есть глобальные массивы и их зануление, хочется делать return 0/exit(0), есть предподсчёт, тесты/ответы сливаются вместе (и строгий чекер).
## Assert'ы
## Нормальные автотесты
### Unit-тесты
* Сначала - Unit-тесты: проще писать, доказывают корректность функций-кирпичиков, можно на это ориентироваться, можно добавлять тесты на баги.
* Пишутся фреймворком, который генерирует клёвые сообщения (тесты будут часто падать).
* Нашёл баг - написал тест, проверил, что падает, поправил, проверил, что не падает.
* Пишем тесты не по коду, а по интерфейсу (хотя знание о реализации может подсказать, какие есть крайние случаи).
* Лучше не использовать детали реализации, если только ровно их мы не тестируем (счётчики ссылок какие-нибудь).
* Тестируем не циклы, а _сценарии поведения_
* Пишем крайние случаи, чтобы зашло во все строчки, пишем парочку "очевидных" случаев, где всё должно работать (и заодно документируем использование).
* Все тесты должны всегда проходить в корректном коде. Flaky - нахрен или запускать вероятностно.
* Никаких внешних зависимостей (временные файлы, ввод-вывод, см. mock).
* Никакого вывода, на который надо смотреть.
* Никакого неожиданного debug-вывода или warning/error'ов в логах - они массов летят в игнор.
* Заменяют статическую типизацию для Python.
* Запускаются одной кнопкой.
* Не проверят то, про что вы не подумали (крайний случай или не так поняли задание, плохо спроектировали интерфейс и потом его где-то не так используете)
* Покрывают все строчки. В идеале - все пути исполнения, но фиг такое получится в реальном коде. Тестируют крайние случаи (пустые строки/списки, None-ы, NULL-ы, nullptr-ы, деление на ноль и прочая). Чтобы на это уже не отвлекаться на вышестоящих уровнях. Заодно документируют, как что работает. Должны тестировать всякие некорректные аргументы.
* Важно проверять, что тесты работают: написали, сломали, проверили, починили, проверили. TDD идёт примерно отсюда.
* Измерять покрытие кода тестами можно автоматически (https://docs.python.org/devguide/coverage.html)
### Mock'и
* Если надо "сэмулировать" окружение (ввод-вывод) - делаем Mock, который предоставляет тот же интерфейс, и следит за происходящим. Можно посчитать число вызовов и параметры.
* Тоже есть фреймворки.
* Важно понимать, что тут мы тестируем именно взаимодействие юнита с остальными в предположении, что остальные хороши. Поэтому и подсовываем Mock. А что, если он захочет по-другому взаимодействовать? Нехорошо.
* Лучше изолировать ФС и прочее в отдельные простые блоки кода (пример с find_duplicates).
### Интеграционные
* По возможности тоже пишутся внутри языка и не имеют внешних зависимостей.
* Это разделение произвольно.
### Функциональные
* Могут поднимать всю систему целиком и гонять запросы.

### Производительность
I/O операции с диском, сеть, профайлер https://docs.python.org/3.6/library/profile.html

# Misc
Показать соотношение между размером кода и тестов на примере redis_cache_test.cxx

Метрики кода: цикломатическая сложность, метрики Холстеда. 
