# Ошибки в программах
## Типы ошибок
### Опечатки
< и <=, не та переменная (i и j), не та константа (10 и 100).
### Нарушение инвариантов
Не доказали бинпоиск, ошибка на единицу.
### Неверное использование сторонней функции (путаница)
Не так используем стороннюю функцию, случайно вызвали не тот перегруженный оператор, забыли про нетривиальное поведение delete и delete[].
### Остатки от изменений
"Проблема последней строки", меняли инвариант и где-то забыли, выносили константу в параметр и забыли поменять.
### Специфичные проблемы
Частный случай нарушения инвариантов:
* Утечки памяти и ресурсов, double-free
* Обращение по несуществующему указателю
* Переполнение буфера
* Многопоточные проблемы (race condition, deadlock)
### Скорость
* Медленный алгоритм
* Много запросов к узкому месту (жёсткий диск, сеть, БД, ядро ОС)
* Полная блокировка и один поток вместо много
### Уязвимости
* Инъекции SQL/XSS
* Неверная обработка плохих входных данных (обычно не подумали, что такое бывает)
* Timing-аттаки и прочее веселье, про которое точно не подумали :)
### Несоответствие требованиям
* Код делает не то, что надо, не соответствует требованиям в голове разработчика (минимум вместо максимума)
* Требования не соответствуют заданию (неверно перевели легенду задачи в требования)
* В легенде задачи баг и она не соответствует здравому смыслу (ошиблись в составлении ТЗ, получили, что имя пользователя секретно, а пароль - нет)

## Почему ошибки важно обнаруживать и исправлять
### Общие слова
* В универе - задание не зачтут.
* В своём проекте - получатся неверные результаты или потеряются данные пользователей.
* В бизнесе - потеря репутации компании и денег, клиентам нужен доступный сервис и доступные данные только кому надо (поэтому уязвимости - это плохо).
* Ссылка на Bumblebee:
https://github.com/MrMEEE/bumblebee-Old-and-abbandoned/commit/a047be85247755cdbe0acce6f1dafc8beb84f2ac
https://github.com/MrMEEE/bumblebee-Old-and-abbandoned/issues/123
### Ужастики
* https://en.wikipedia.org/wiki/Stuxnet
* https://en.wikipedia.org/wiki/Cluster_(spacecraft)#Launch_failure
* Соцсети - это важно, связь.
* Люди умирают (https://ru.wikipedia.org/wiki/Therac-25#.D0.97.D0.B0.D0.BC.D0.B5.D1.87.D0.B5.D0.BD.D0.BD.D1.8B.D0.B5_.D0.BE.D1.88.D0.B8.D0.B1.D0.BA.D0.B8 , https://en.wikipedia.org/wiki/2009%E2%80%9311_Toyota_vehicle_recalls)
### Управление рисками
* Ошибки бывают критические и не очень. Некоторые стоит искать и исправлять раньше других.
* Некоторые задерживают выпуск кода, некоторые - нет.

## Как обнаруживают ошибки
https://vk.com/note11191_9367282 - кто слышал про Андрея Лопатина? Николая Дурова? Павла Дурова? Все в ВК и Телеграме, а первые два - двойные чемпионы ACM ICPC.
"Протестировать задачу – это не значит ввести парочку тестов из условия и еще абы какие."
"Тестирование – это технологический процесс, направленный на выявление ошибок."
### Статически
* Компилятор
* Типы данных (чем строже, тем круче)
* Изолируем трудные участки и пристально смотрим на них
* Стиль кода (запрещаем использовать опасные конструкции, перегружать операторы, добавляем хорошие имена)
* Статические анализаторы (https://habrahabr.ru/company/pvs-studio/blog/313670/), хороши против копипаста и популярных ошибок (копипаст)
* Формальные системы доказательств (впрочем, в них же тоже могут быть баги; можно случайно доказать не то, что надо - это же тоже код, просто со строгими типами)
### Динамически
* Запускаем программу (или кусок), смотрим, что получилось, сравниваем с ожидаемым поведением
* Один запуск обычно называют "тестом"
* Тестов конечно, сценариев поведения обычно счётно, поэтому все случаи не разберём, надеемся, что конечного числа хватит.
* Можно тестировать старые баги (регрессии) - они очень любят возвращаться, особенно если программист тогда запутался.
* Можно запускать руками или автоматически.
* Можно полностью описывать заранее, можно генерировать автоматически.
* Можно проверять: корректность ответа, время отклика/работы, потребляемую память, искать произошедшие утечки и race conditions (sanitizers, valgrind)
### Программист и тестировщик
* Иногда разделяют программистов и тестировщиков.
* Программист пишет код и знает всё про реализацию, он её может протестировать. И мелкий интерфейс тоже.
* Тестировщик знает всё про продукт и должен тестировать его. С кодом не обязательно знаком. Тестирует продукт в целом, стабильность, очевидность, можно скорость. Могут писать скрипты для автоматизированных тестов (Selenium для веб-приложений), которые от кода вообще не зависят.
* Картинка: http://bash.im/comics/20080111
* "Протестируйте калькулятор". Кто сказал про сложение первым? Это правильно. Популярные случаи как раз и надо тестировать (см. приоритетность багов). На олимпиадах, кстати, тоже.
* Почему-то не любят тестировщиков и тесты в принципе. На самом деле: http://risovach.ru/upload/2014/12/mem/mudrec-monah_68447818_orig_.jpg
* В будущем надо тестировать продукт, а не искать ошибки: https://habrahabr.ru/post/149903/
### Логирование
* Помогает найти проблему, когда она уже свершилась, и непонятны обстоятельства
* Обычно делают сообщения на несколько уровней: debug, info, warning, error и включают тонкие уровни при отладке. Тогда не надо комментировать код.
* Если использовать либу вроде slf4j, то можно очень удобно конфигурировать, где что включать (в Java - в каких файлах какой уровень), настраивать формат вывода, перенаправлять логи куда хочешь и так далее.
* Если вы где-то пишите debug-вывод - поздравляю, это оно. Только логгинг лучше делать с уровнями и библиотекой, чтобы его можно было просто отключать. Мне помогало на сложных проектах: "так, клиент подключился, получил такие-то данные, отправил такие-то, а второй клиент получил только часть и повис вот тут, упс".
* В сочетаниии с автотестами позволяет вообще не расчехлять отладчик - всё и так понятно по логам.
* Пример логов: https://github.com/yeputons/spbau-java-course-torrent-term4/blob/master/src/main/java/net/yeputons/spbau/spring2016/torrent/tracker/TrackerServer.java

# Тесты
## Типы тестов
### Ручное тестирование
Ввод руками, проверка глазами.
Обычно так тестировать можно UI, ввод-вывод, интеграцию со сторонними сервисами, сетевые игрушки (но не что-то, где надо быстро реагировать, и не многопоточность - там фиг отрегулируешь).
### Автоматическое тестирование
Ввод из фиксированного файла или генерируется.
Проверка полностью автоматически, глазами ничего читать не надо.
Обычно используют фреймворки, которые запускают тесты параллельно, с красивыми отчётами и сообщениями об ошибках.
### Полуавтоматическое
Ввод данных руками или проверка частично глазами.
Промежуточный компромисс для своих наколенных проектов.
### Встроенные в код
assert'ы и сообщения в лог: лучше последнее, особенно в надёжных системах (упавший процесс на самолёте - плохая идея, пусть лучше часть функционала отключит до перезагрузки, костыль, но что поделать).
Позволяют проверять вообще всё что угодно, но надо аккуратно. А то окажется, что высота не бывает отрицательной над мёртвым морем (байка про истребители Израиля).

## Чем хороши автотесты
* Такие тесты можно запускать постоянно в CI на отдельном кластере, быстро сообщают об ошибках.
* Огромное число багов ловится тестами, которые легко написать.
* Документируют и фиксируют ожидаемое поведение (привет из гугла - там если к коду тестов нет, то их пишут, просто чтобы проверить, что ничего не поломали при изменениях).
* Если вбиваете на каждый запуск что-то - очевидно, что можно автоматизировать и будет во всяком случае быстрее.
* Обнажаются изъяны архитектуры: если что-то сложно протестировать, то там наверняка прячется куча зависимостей и инвариантов.

## Примеры
### Полуавтоматическое
Задачи на олимпиадах при запуске руками.
Показали ввод - получили вывод. Написали скрипт, который запустит код X раз на всех тестах, проверит ответы.
На олимпиадах вы постоянно запускаете решение на конкретном тесте, поэтому - мультитест и diff на файлик с ответом (потому что управлять несколькими файликами сложнее).
Обычно возникают проблемы (все хорошо лечатся): читать до EOF, есть глобальные массивы и их зануление, хочется делать return 0/exit(0), есть предподсчёт, тесты/ответы сливаются вместе (и строгий чекер).
### Автоматические
Стресс-тест на контестах.
Показать, как я тестирую Haskell.
Быстро, не трачу мозг, не думаю о том, что что-то забыл, могу сосредоточиться на содержательной части - стиль кода, а не корректность.
Показать значок Travis на странице: https://github.com/pagespeed/mod_pagespeed/
А в гугле автоматически запускаются вообще все тесты при любом изменении кода в любой точке кода гугла. И тесты пишем.

## Типы автоматических тестов
### По знанию кода
White box знают всё про код и могут тестировать реализацию конкретной функции и тестируют баги конкретной реализации, black box тестируют только интерфейс (их можно написать по заданию по парадигмам :).
### По гранулярности
* Unit - отдельные интерфейсы, без зависимостей (с mock'ами), точно указывают место ошибки.
* Интеграционные - как взаимодействуют несколько юнитов между собой 
* Функциональные - вся система целиком
* Возможны промежуточные варианты
Обычно unit-тесты покрывают вообще весь код, интеграционные чуть меньше (без крайних случаев), функциональные - ещё меньше (основные сценарии).
### По целям
* Сценарии поведения
** Просто среднестатистическое использование: создали объект, что-то сделали, проверили.
** Крайние случаи
** Обработка ошибок
* Регрессионные: на старые баги
* Проверка типов в динамических языках
* Соответствие решения задаче: только black box, проверяем глобально интерфейс, могут писать тестировщики.
* Стабильность: стресс-тест, рандомный ввод
* Скорость работы: "линия, а не квадрат". Можно графики строить.
* Smoke-тесты: "воткнули в розетку, дым не пошёл". Просто какие-то тупые простые тесты, которые ловят совсем полную чушь.
### По качеству
* Воспроизводимые
* Flaky, нестабильные, вероятностные (например, на время или сетевое взаимодействие), обычно разрешают X% падений определённого вида (только TL, но не WA, скажем)

## Какие тесты писать и когда
1. Если код простой, небольшой, а вы самоуверенны - то пишем сразу функциональные тесты (всё равно пригодятся).
2. Если код относительно независимый или из большого количества кусков - пишем сначала юнит-тесты (потому что при баге в одном месте код сложится, как карточный домик, и надо будет искать место ошибки, а если нет фундамента - не найти), потом - интеграционные, а потом - функциональные.
### Функциональные тесты
1. Смотрим, какие тесты мы вводим руками.
2. Ставим задачу формально.
3. Автоматизируем те тесты, что ввели руками. Только их, ничего больше. Пусть ни фига не проверяют, но хоть что-то. Мы хотим сначала инфраструктуру. Это мы уже получили smoke-тесты.
4. Проверяем, что проходят.
5. Ломаем код.
6. Проверяем, что не проходят. Если прошло - добавляем тестов или ищем в них баг (в самих тестах). Это важно!
7. Чиним код обратно.
8. Добавляем тесты, которые было лень вводить. Например, макстесты - это уже должно быть просто.
### Юнит-тесты
* Обычно пишутся на базе какого-то фреймворка (прямо на языке, на котором написанна программа), который позволяет запускать не все тесты, а часть (чтобы было чуть быстрее) и рисовать красивые сообщения об ошибках и отчёты.
* Могут вызывать функции программы напрямую.
1. Смотрим, какие куски кода наиболее опасны с точки зрения багов (там, где живут инварианты и которые активно используются).
2. Пишем для них формальную постановку задачи.
3. Добавляем assert'ы на входные данных.
4. Добавляем assert'ы на выходные данные и, если это тестирует не все случаи (например, не тестирует "элемент не найден") - добавляем юнит-тесты.
5. Добавляем юнит-тесты, которые запустят на известных тестах (в том числе с крайними случаями) и проверят ответ руками при помощи побитового сравнения. А то вдруг вы в assert'е набажили.
6. Запускаем юнит-тесты после каждого изменения кода.
7. Намеренно ломаем код в тех местах, которые тестировали, проверяем, что тесты ловят изменение. Это важно!
* У меня в сложных проектах юнит-тесты идут практически сразу после любого кода, чтобы я был уверен, что я ничего не сломал, и мне не приходилось думать, как протащить данные к вон-той-вот-внутренней-функции.
* Есть другой подход - TDD - сначала тесты, они ломаются, а потом реализация, которая их чинит.
### Интеграционные тесты
Различие с unit-тестами довольно условное, больше по размеру, чем по смыслу. Пишутся обычно точно так же, возможно, добавляется какое-то внешнее окружение (например, "поднять внешний сервер БД, с которым взаимодействует адаптер к этой самой БД")
### Тесты производительности
* Только если клиенты жалуются на скорость
* Макс тест по размеру - не всегда худший для кода, надо пробовать разные
* Большой тест всегда нужен, на маленьких наверняка будет тестироваться скорость не того
* Время работы недетерминированно, лучше либо давать допуск, либо строить графики и смотреть на них глазами (а лучше и то, и другое)
* Обычно очень помогает вести какую-нибудь статистику внутри кода (количество операций с деревом, например)
* Также можно рубануть профайлером по функциям или строчкам кода: https://docs.python.org/3.6/library/profile.html
* Часто бывает, что тормозит не код, а взаимодействие между кодом: I/O, сеть, обращения к стороннему серверу (классика - сто тысяч запросов к БД вместо одного с JOIN'ом)
### Размер тестов
* Моя стажировка летом: 735 строк в https://github.com/pagespeed/mod_pagespeed/blob/master/pagespeed/system/redis_cache.cc
* Тесты:
1. https://github.com/pagespeed/mod_pagespeed/blob/master/pagespeed/system/redis_cache_test.cc
2. https://github.com/pagespeed/mod_pagespeed/blob/master/pagespeed/system/redis_cache_cluster_setup.cc
3. https://github.com/pagespeed/mod_pagespeed/blob/master/pagespeed/system/redis_cache_cluster_test.cc
* Очень-очень хорошие тесты - это много кода, но в серьёзных ситуациях оно того стоит.
* Можно иногда скатываться к полуавтоматическим тестам, тогда сильно проще жить.

# Юнит-тесты
## Библиотеки
* Помогают обнаруживать ошибки и рисовать отчёты: JUnit и аналоги, pytest, Google Test
* Обычно стараются минимизировать объём boilerplate, чтобы разработчики писали побольше тестов. pytest в этом месте - огонь.
* Хорошие сообщения об ошибках упрощают их исправление и это становится приятным занятием.
* Показать пример кода, автообнаружение тестов, отчёт об ошибках.
* Показать пример на Haskell (просто размер и вывод).
* Рассказать, что в JUnit требуется явно указывать, что за сравнение идёт, чтобы было хорошее сообщение:
https://github.com/yeputons/DbFall2013/blob/master/src/test/java/net/yeputons/cscenter/dbfall2013/engines/SimpleEngineTest.java

## Как принято писать
* Хороший: зависит только от интерфейса (а не по коду), нет зависимостей и вспомогательных функций
* Хотя знание кода может подсказать, какие дополнительные тесты интерфейса надо написать.
* Если совсем надо протестировать именно реализацию - может, распилить функцию на несколько? Ну или можно написать честный white-box тест, но его лучше от остальных отделить. Например, тестирование подсчёта ссылок в умном указателе.
* Написан в стиле "настроили-действие-проверили"
* Нашли баг - сразу написали тест, *проверили падение*, поправили, *проверили работу*
* Тестируем не циклы и if'ы реализации, а "сценарии поведения": как этот объект/функцию будут использовать клиенты в реальном мире.

## Что unit-тестировать
* Полное покрытие строк кода (можно мерять автоматически: https://docs.python.org/devguide/coverage.html). Это заменит статическую типизацию для Python и заодно половит кучу крайних случаев. В идеале - все пути исполнения, но фиг такое получится в реальном коде.
* "Очевидные" случаи
* Все крайние случаи (пустые строки/списки, None-ы, NULL-ы, nullptr-ы, деление на ноль и прочая) - потом вы их уже тестировать не будете
* Всякие некорректные аргументы.
* Заодно документируем "как использовать библиотеку", это документация никогда не устареет
* Полезно писать такие тесты к старому коду, у которого тестов не было: тогда мы фиксируем старое поведение и его точно не поломаем, и не поломаем зависимый код (привет из гугла)

## Требования к unit-тестами
* Все тесты всегда проходят в корректном коде. Нестабильные тесты лесом - делаем детерминированными (потому что фиг поймаешь ошибку). Особенно весело в многопоточных программах
* Все внешние зависимости очень изолированны внутри кода теста (максимум - временный файл создать при помощи фреймворка, чтобы он потом удалился), никаких "поднимите SQL-сервер там-то". Если хочется - см. mock'и чуть позже
* Никаких зависимостей от других unit'ов - тестируем отдельно. Если надо - см. mock
* Отрабатывает за миллисекунды, чтобы можно было запустить сотни тестов за секунду
* Никакого вывода, на который надо глазами смотреть (это общее требование к автотестам). Сообщения об ошибках в логе - либо ожидаемые, либо сигналят об ошибке. Иначе кто-нибудь либо забудет прочитать логи, либо забьёт, потому что там куча сообщений не по делу.
* Запускаются одной кнопкой (при правильном фреймворке), чтобы всегда можно было пустить
* Важно проверять, что тесты работают: написали, сломали, проверили, починили, проверили. TDD идёт примерно отсюда.

## Проблемы с unit-тестами
* Не проверят то, про что вы не подумали (крайний случай или не так поняли задание, плохо спроектировали интерфейс и потом его где-то не так используете)
* Не проверят, что функции друг про друга предполагают одно и то же (для этого есть интеграционные)
* Не проверят, что программа в целом адекватна (для этого есть функциональные)
* Поэтому даже если все юнит-тесты проходят - ещё не факт, что всё корректно. Но очень хорошее приближение, из которого намного проще что-то исправлять

## Что делать с зависимостями
* Если компонент зависит от реализации остальных, в юнит-тестах.
* Библиотека для Mock'ов вроде https://docs.python.org/3/library/unittest.mock.html - создаёт "фиктивные объекты", у которых тест задаёт поведение. Сначала рассказываем mock'у, каких вызовов ждать и как реагировать, потом используем.
* Надо осторожно с mock'ами, потому что мы влезаем в детали реализации ("сколько раз вызывается такой-то метод" или "с какими аргументами")
* Можно делать mock на внешние зависимости: базу данных, файлы, сеть. В динамических языках просто monkey patching (есть в библиотеках), а в статических надо аккуратно выносить интерфейсы и не делать глобальных или статических переменных (потому что их не запатчишь). Доведение до абсолюта - dependency injection на всё на свете, но не в этом курсе.
* Пример с mock'ом для страницы.
* Пример с monkey patch стандартного ввода.

# Практика
* `pip install pytest`
* Качаем скрипты первого задания (без wordcount) с https://www.dropbox.com/sh/l7zh2fquflldw9y/AAArxvYNcScTLqPJ5IL355EZa?dl=0
* Пишем юнит-тесты вместе, не оглядываясь на реализацию (типа TDD).
* Пишем реализацию, настраиваем хоткей в редакторе, тестируем, итерируемся.
* Решаем задание: "написать калькулятор для выражений вида <<сложение>>, <<деление>> и <<синус от этих двух>>". Надо разделить на функции, написать юнит тесты, и - самое главное - функциональный тест с monkey patch и StringIO (mock)
